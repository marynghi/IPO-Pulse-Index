# -*- coding: utf-8 -*-
"""IPO Pulse_Separated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V1sTBOxUurnbm-lN9zbt7fzC1lML6q-G

# Prep Index Inputs
"""

import pandas as pd

file_path_oecd = '/content/drive/MyDrive/IPO Pulse/OECD.SDD.STES,DSD_STES@DF_CLI,+SWE.M.CCICP+BCICP...AA.IX..H.csv'
df_oecd = pd.read_csv(file_path_oecd)
display(df_oecd.head())

file_path_excel = '/content/drive/MyDrive/IPO Pulse/omx_stoxx.xlsx'
df_omx = pd.read_excel(file_path_excel, sheet_name='Sheet1')
display(df_omx.head())
df_stoxx = pd.read_excel(file_path_excel, sheet_name='Sheet2')
display(df_stoxx.head())

file_path_excel = '/content/drive/MyDrive/IPO Pulse/Valuation_input.xlsx'
df_val = pd.read_excel(file_path_excel)
display(df_val.head())

"""## Volatility & Returns DF"""

import numpy as np

# Ensure 'Pricing Date' is in datetime format and set as index for df_omx
df_omx_daily = df_omx.copy()
df_omx_daily['Pricing Date'] = pd.to_datetime(df_omx_daily['Pricing Date'].astype(str))
df_omx_daily = df_omx_daily.set_index('Pricing Date')

# Calculate daily returns (using log returns)
df_omx_daily.loc[:, 'OMXS30_log_ret'] = np.log(df_omx_daily['OMXS30']).diff()

# Calculate 21-trading-day rolling standard deviation of daily log returns (annualized)
df_omx_daily.loc[:, 'OMXS30_rv_21d'] = df_omx_daily['OMXS30_log_ret'].rolling(window=21).std() * np.sqrt(252)  # annualized

# Convert to month-end by resampling and taking the last value of the 21-day rolling volatility
df_omx_monthly = (df_omx_daily.resample('ME')['OMXS30_rv_21d']
              .last()
              .to_frame('OMXS30_rv_1m'))

# Calculate 6-month trailing average of 1m realized vol
df_omx_monthly.loc[:, 'omxs30_rv_1m_6mma'] = df_omx_monthly['OMXS30_rv_1m'].rolling(window=6).mean()

display(df_omx_monthly.head(50))

# Ensure 'Pricing Date' is in datetime format and set as index for df_stoxx
df_stoxx_daily = df_stoxx.copy()
df_stoxx_daily['Pricing Date'] = pd.to_datetime(df_stoxx_daily['Pricing Date'].astype(str))
df_stoxx_daily = df_stoxx_daily.set_index('Pricing Date')

# Resample the daily data to month-end and take the last price of the month
df_stoxx_monthly = df_stoxx_daily.resample('ME')['SXXP'].last().to_frame()

# Calculate Year-over-Year growth for STOXX 600 on monthly data
df_stoxx_monthly.loc[:, 'stoxx600_price_yoy'] = df_stoxx_monthly['SXXP'].pct_change(periods=12) * 100

display(df_stoxx_monthly.head(20))

"""## Confidence DF"""

df_bcicp = df_oecd[df_oecd['MEASURE'] == 'BCICP'].copy()
df_ccicp = df_oecd[df_oecd['MEASURE'] == 'CCICP'].copy()
display(df_bcicp.head())
display(df_ccicp.head())

df_bcicp = df_bcicp[['TIME_PERIOD', 'OBS_VALUE']]
df_ccicp = df_ccicp[['TIME_PERIOD', 'OBS_VALUE']]
display(df_bcicp.head())
display(df_ccicp.head())

df_omx['Pricing Date'] = pd.to_datetime(df_omx['Pricing Date'].astype(str)).dt.to_period('M')
df_stoxx['Pricing Date'] = pd.to_datetime(df_stoxx['Pricing Date'].astype(str)).dt.to_period('M')

display(df_omx.head())
display(df_stoxx.head())

"""## Capital DF"""

file_path_ipo_capital = '/content/drive/MyDrive/IPO Pulse/IPO capital.xlsx'
xls = pd.ExcelFile(file_path_ipo_capital)
sheet_names = xls.sheet_names

print(f"Found sheets: {sheet_names}")

capital_dfs = {}
for sheet_name in sheet_names:
    df = pd.read_excel(xls, sheet_name=sheet_name)
    capital_dfs[sheet_name] = df
    print(f"Displaying head of '{sheet_name}':")
    display(df.head())

def prep_index(df, date_col="Date"):
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], format='%b %Y')
    df = df.set_index(date_col).sort_index()
    return df

# Update the capital_dfs dictionary with the prepped dataframes
capital_dfs['dry_powder']       = prep_index(capital_dfs['dry_powder'], date_col='Date')        # annual or quarterly
capital_dfs['capraise_early_nor'] = prep_index(capital_dfs['capraise_early_nor'], date_col='Date')  # quarterly
capital_dfs['capraise_early_eu']  = prep_index(capital_dfs['capraise_early_eu'], date_col='Date')   # quarterly
capital_dfs['capraise_late_eu']   = prep_index(capital_dfs['capraise_late_eu'], date_col='Date')    # quarterly
capital_dfs['general_pe_nor']     = prep_index(capital_dfs['general_pe_nor'], date_col='Date')      # quarterly

# Initialize a new dictionary to store the monthly resampled dataframes
capital_dfs_monthly = {}

# Iterate through each dataframe in capital_dfs
for name, df in capital_dfs.items():
    # Resample to month-end ('ME') and apply forward-fill
    # For 'dry powder' which can be annual or quarterly, resampling to 'ME' then ffill() handles this
    # For others, which are quarterly, this also works correctly.
    resampled_df = df.resample('ME').ffill()
    capital_dfs_monthly[name] = resampled_df

# Display the head of one of the resampled dataframes to verify
print("Displaying head of 'dry powder' (monthly and forward-filled):")
display(capital_dfs_monthly['dry_powder'].head())

print("Displaying head of 'capraise_early_nor' (monthly and forward-filled):")
display(capital_dfs_monthly['capraise_early_nor'].head())

# Adjust column names here to match your dfs
dry_early = capital_dfs_monthly['dry_powder']["Early Stage"]   # e.g. dry powder early-stage
dry_late  = capital_dfs_monthly['dry_powder']["Expansion / Late Stage"]    # e.g. dry powder late-stage

early_nor = capital_dfs_monthly['capraise_early_nor']["Capital Raised"]      # Nordic early-stage capital raised
early_eu  = capital_dfs_monthly['capraise_early_eu']["Capital Raised"]       # EU early-stage capital raised
late_eu   = capital_dfs_monthly['capraise_late_eu']["Capital Raised"]        # EU late-stage capital raised
pe_nor    = capital_dfs_monthly['general_pe_nor']["Capital Raised"]     # Total Nordic PE capital raised

def yoy(series):
    return series.pct_change(12)

# Capital raised / PE flows → YoY change
early_nor_yoy = yoy(early_nor)
early_eu_yoy  = yoy(early_eu)
late_eu_yoy   = yoy(late_eu)
pe_nor_yoy    = yoy(pe_nor)

# Dry powder: you can keep as level (we’ll z-score later)
dry_early_lvl = dry_early
dry_late_lvl  = dry_late

def zscore(s):
    return (s - s.mean()) / s.std(ddof=0)

z_early_nor = zscore(early_nor_yoy)
z_early_eu  = zscore(early_eu_yoy)
z_late_eu   = zscore(late_eu_yoy)
z_pe_nor    = zscore(pe_nor_yoy)

z_dry_early = zscore(dry_early_lvl)
z_dry_late  = zscore(dry_late_lvl)

# A. Early/Late ratio (use EU late-stage because you lack Nordic late-stage raise)
ratio_early_late = early_nor / late_eu

# B. Dry powder / Capital raised ratio
# Choose late-stage dry powder divided by total Nordic PE raise (broad liquidity ratio)
ratio_dry_cap = dry_late / pe_nor

# Z-score ratios (ratios behave best once z-scored)
z_ratio_early_late = zscore(ratio_early_late)
z_ratio_dry_cap    = zscore(ratio_dry_cap)

"""### Build Cap Index"""

early_components = pd.concat([
    z_early_nor,
    z_early_eu,
    z_dry_early,
    z_ratio_early_late  # ratio included
], axis=1)

early_components.columns = [
    "z_early_nor",
    "z_early_eu",
    "z_dry_early",
    "z_ratio_early_late"
]

EarlyStage_Index = early_components.mean(axis=1, skipna=True)

# Optionally lag by 12 months
EarlyStage_Index_lagged = EarlyStage_Index.shift(12)

late_components = pd.concat([
    z_late_eu,          # EU late-stage (until 2024)
    z_dry_late,         # Nordic dry powder (full)
    z_pe_nor,           # Nordic PE activity
    z_ratio_dry_cap     # liquidity ratio
], axis=1)

late_components.columns = [
    "z_late_eu",
    "z_dry_late",
    "z_pe_nor",
    "z_ratio_dry_cap"
]

LateStage_Index = late_components.mean(axis=1, skipna=True)

IPO_Pipeline_Proxy = 1 * LateStage_Index + 0 * EarlyStage_Index_lagged
IPO_Pipeline_Proxy.name = "IPO_Pipeline_Proxy"

ipo_proxy_df = pd.DataFrame({
    "EarlyStage_Index": EarlyStage_Index,
    "EarlyStage_Index_lagged": EarlyStage_Index_lagged,
    "LateStage_Index": LateStage_Index,
    "IPO_Pipeline_Proxy": IPO_Pipeline_Proxy,
    "z_late_eu": z_late_eu, # Added z_late_eu
    "z_dry_late": z_dry_late # Added z_dry_late
})

display(ipo_proxy_df.head())

"""## Valuation DF"""

# Make a copy to avoid modifying the original df_val if it's used elsewhere
df_val_processed = df_val.copy()

# 1. Convert 'Dates' to datetime format
df_val_processed['Dates'] = pd.to_datetime(df_val_processed['Dates'], format='%b-%d-%Y')

# 2. Clean 'TEV/EBITDA': replace comma with dot for decimals and convert to numeric
df_val_processed['TEV/EBITDA'] = df_val_processed['TEV/EBITDA'].str.replace(',', '.', regex=False).astype(float)

# 3. Set 'Dates' as index
df_val_processed = df_val_processed.set_index('Dates')

# 4. Resample to month-end ('ME') and calculate the mean of 'TEV/EBITDA'
df_val_monthly = df_val_processed['TEV/EBITDA'].resample('ME').mean().to_frame()
df_val_monthly.columns = ['TEV/EBITDA_monthly_avg']

display(df_val_monthly.head())
display(df_val_monthly.tail())

"""## Merge all input DFs"""

# Rename TIME_PERIOD to Pricing Date in df_bcicp and df_ccicp
df_bcicp = df_bcicp.rename(columns={'TIME_PERIOD': 'Pricing Date'})
df_ccicp = df_ccicp.rename(columns={'TIME_PERIOD': 'Pricing Date'})

# Convert 'Pricing Date' in df_bcicp and df_ccicp to period[M]
# Handle potential re-run scenario where 'Pricing Date' might already be PeriodDtype
# Convert to timestamp first if it's a PeriodDtype, then to Period[M]
if isinstance(df_bcicp['Pricing Date'].dtype, pd.PeriodDtype):
    df_bcicp['Pricing Date'] = df_bcicp['Pricing Date'].dt.to_timestamp().dt.to_period('M')
else:
    df_bcicp['Pricing Date'] = pd.to_datetime(df_bcicp['Pricing Date']).dt.to_period('M')

if isinstance(df_ccicp['Pricing Date'].dtype, pd.PeriodDtype):
    df_ccicp['Pricing Date'] = df_ccicp['Pricing Date'].dt.to_timestamp().dt.to_period('M')
else:
    df_ccicp['Pricing Date'] = pd.to_datetime(df_ccicp['Pricing Date']).dt.to_period('M')

# Ensure 'Pricing Date' index in df_omx_monthly and df_stoxx_monthly is converted to period[M] and reset as a column for merging
# Drop redundant index columns if they exist from previous reset_index calls, then reset index
df_omx_monthly = df_omx_monthly.drop(columns=['level_0', 'index'], errors='ignore')
df_omx_monthly = df_omx_monthly.reset_index()
if not isinstance(df_omx_monthly['Pricing Date'].dtype, pd.PeriodDtype):
    df_omx_monthly['Pricing Date'] = df_omx_monthly['Pricing Date'].dt.to_period('M')

df_stoxx_monthly = df_stoxx_monthly.drop(columns=['level_0', 'index'], errors='ignore')
df_stoxx_monthly = df_stoxx_monthly.reset_index()
if not isinstance(df_stoxx_monthly['Pricing Date'].dtype, pd.PeriodDtype):
    df_stoxx_monthly['Pricing Date'] = df_stoxx_monthly['Pricing Date'].dt.to_period('M')

# Prepare ipo_proxy_df for merging
ipo_proxy_df_merged = ipo_proxy_df.reset_index() # Convert 'Date' index to a column
ipo_proxy_df_merged = ipo_proxy_df_merged.rename(columns={'Date': 'Pricing Date'}) # Rename 'Date' to 'Pricing Date'
if not isinstance(ipo_proxy_df_merged['Pricing Date'].dtype, pd.PeriodDtype):
    ipo_proxy_df_merged['Pricing Date'] = ipo_proxy_df_merged['Pricing Date'].dt.to_period('M') # Convert to Period[M]

# Prepare df_val_monthly for merging
df_val_monthly_merged = df_val_monthly.reset_index() # Convert 'Dates' index to a column
df_val_monthly_merged = df_val_monthly_merged.rename(columns={'Dates': 'Pricing Date'}) # Rename 'Dates' to 'Pricing Date'
if not isinstance(df_val_monthly_merged['Pricing Date'].dtype, pd.PeriodDtype):
    df_val_monthly_merged['Pricing Date'] = df_val_monthly_merged['Pricing Date'].dt.to_period('M') # Convert to Period[M]

# Merge the dataframes
merged_df = pd.merge(df_stoxx_monthly, df_omx_monthly, on='Pricing Date', how='outer')
merged_df = pd.merge(merged_df, df_bcicp, on='Pricing Date', how='outer')
merged_df = pd.merge(merged_df, df_ccicp, on='Pricing Date', how='outer')
merged_df = pd.merge(merged_df, ipo_proxy_df_merged, on='Pricing Date', how='outer') # Merge ipo_proxy_df
merged_df = pd.merge(merged_df, df_val_monthly_merged, on='Pricing Date', how='outer') # Merge df_val_monthly

# Rename OBS_VALUE columns
merged_df = merged_df.rename(columns={
    'OBS_VALUE_x': 'BCICP', # Based on the merge order
    'OBS_VALUE_y': 'CCICP'  # Based on the merge order
})

display(merged_df.head())

"""## Cleaning"""

merged_df_cleaned = merged_df.dropna().copy()
num_rows_after_dropna = len(merged_df_cleaned)

print(f"Number of rows after removing NaN values: {num_rows_after_dropna}")
display(merged_df_cleaned.head())

"""# Prep IPO Activity DF


"""

file_path_ipo_counts = '/content/drive/MyDrive/IPO Pulse/IPO counts.xlsx'
df_ipo_counts = pd.read_excel(file_path_ipo_counts)
display(df_ipo_counts.head())

df_ipo_counts = df_ipo_counts.rename(columns={
    'Announced Date\nMM/dd/yyyy': 'Announced Date',
    'Total Transaction Value\n(€M)': 'Total Transaction Value (M EUR)',
    'Offering Announcement Date\nMM/dd/yyyy': 'Offering Announcement Date',
    'Trade Date\nMM/dd/yyyy': 'Trade Date',
    'Industry Group\n(Target/Issuer)': 'Industry Group',
    'Primary Industry\n(Target/Issuer)': 'Primary Industry',
    'SIC Code\n(Target/Issuer)': 'SIC Code',
    'IPO Aftermarket Performance\n(%) (One day)': 'IPO Aftermarket Performance (1 day)',
    'IPO Aftermarket Performance\n(%) (One week)': 'IPO Aftermarket Performance (1 week)',
    'IPO Aftermarket Performance\n(%) (Three month)': 'IPO Aftermarket Performance (3 month)',
    'IPO Aftermarket Performance\n(%) (One month)': 'IPO Aftermarket Performance (1 month)',
    'IPO Aftermarket Performance\n(%) (Six month)': 'IPO Aftermarket Performance (6 month)',
    'IPO Aftermarket Performance\n(%) (One year)': 'IPO Aftermarket Performance (1 year)'
})

display(df_ipo_counts.head())

unique_exchanges = df_ipo_counts['Exchange'].unique()
print(unique_exchanges)

"""## Exchange filter"""

exchanges_to_keep = ['OM', 'XSAT', 'NGM']
df_ipo_sweden = df_ipo_counts[df_ipo_counts['Exchange'].isin(exchanges_to_keep)].copy()
display(df_ipo_sweden.head())

num_rows = df_ipo_sweden.shape[0]
print(f"The df_ipo_sweden DataFrame has {num_rows} rows.")

# Ensure 'Announced Date' is in datetime format
df_ipo_sweden['Announced Date'] = pd.to_datetime(df_ipo_sweden['Announced Date'])

# Group by month-end and count IPOs
df_count_swed = df_ipo_sweden.groupby(pd.Grouper(key='Announced Date', freq='ME')).size().reset_index(name='IPO_Count')

# Convert 'Announced Date' to period[M] for consistent merging later
df_count_swed['Announced Date'] = df_count_swed['Announced Date'].dt.to_period('M')

# Rename the date column to 'Pricing Date' for consistency with other dataframes
df_count_swed = df_count_swed.rename(columns={'Announced Date': 'Pricing Date'})

display(df_count_swed.head())

"""## Merge with input DF"""

merged_df = pd.merge(merged_df, df_count_swed, on='Pricing Date', how='left')
display(merged_df.head())

# Convert 'Pricing Date' to datetime objects for comparison
merged_df['Pricing Date'] = merged_df['Pricing Date'].dt.to_timestamp()

# Filter rows where 'Pricing Date' is 1999-07 or later
merged_df = merged_df[merged_df['Pricing Date'] >= '1999-07-01']

# Convert 'Pricing Date' back to Period[M] for consistency with previous operations
merged_df['Pricing Date'] = merged_df['Pricing Date'].dt.to_period('M')

display(merged_df.head())

"""# Trends & Correlation"""



"""# Composite Index

## Choosing inputs

## Normalizing and constructing
"""

component_cols = [
    "stoxx600_price_yoy",
    "omxs30_rv_1m_6mma",
    "BCICP",
    "CCICP",
    "TEV/EBITDA_monthly_avg",
    "z_late_eu",
    "z_dry_late"
]

# Extract only these into a new DataFrame
components = merged_df[component_cols].copy()

# Set 'Pricing Date' from merged_df_cleaned as index, converting to timestamp first
components.index = merged_df['Pricing Date'].dt.to_timestamp()
components = components.sort_index()

def zscore(s):
    return (s - s.mean()) / s.std(ddof=0)

z_df = components.apply(zscore)
z_df.columns = ["z_" + c for c in z_df.columns]

z_df["RawComposite"] = z_df.mean(axis=1, skipna=True)
RawComposite = z_df["RawComposite"]

"""# Fitting"""

# Define norm based on z_df after fixing its index
norm = z_df.copy()
norm = norm.rename(columns={'RawComposite': 'raw_composite'}) # Rename if needed

# Resample to quarterly frequency (end of quarter) and calculate the mean
composite_q = norm['raw_composite'].resample('QE').mean().to_frame()

display(composite_q.head())

# Convert 'Pricing Date' in merged_df_cleaned to Timestamp and set as index for resampling
# Make a copy to avoid SettingWithCopyWarning if merged_df_cleaned is used elsewhere
ipo_data_for_q_resampling = merged_df.copy()
ipo_data_for_q_resampling['Pricing Date'] = ipo_data_for_q_resampling['Pricing Date'].dt.to_timestamp()
ipo_data_for_q_resampling = ipo_data_for_q_resampling.set_index('Pricing Date')

# Resample the 'IPO_Count' to quarterly frequency (end of quarter) and sum the counts
ipo_count_q = ipo_data_for_q_resampling['IPO_Count'].resample('QE').sum().to_frame()
ipo_count_q.columns = ['IPO_Count'] # Ensure column name is consistent

# Now perform the merge
align = pd.merge(composite_q, ipo_count_q, left_index=True, right_index=True, how='inner')
display(align.head())

from sklearn.linear_model import LinearRegression

# Define the independent variable (X)
X = align[['raw_composite']]

# Define the dependent variable (y)
y = align['IPO_Count']

# Instantiate and fit the Linear Regression model
model = LinearRegression()
model.fit(X, y)

# Print the coefficient of the model
print(f"Coefficient: {model.coef_[0]}")

align['adj_composite_q'] = model.predict(X)
display(align.head())

"""## Pulse Index
Benchmark by year and align by month
"""

# Choose a base year you have full data for
base_mask = (RawComposite.index.year == 1999)  # or 2015, or first full year in your sample
base_mean = RawComposite[base_mask].mean()

PulseIndex = 100 * RawComposite / base_mean
PulseIndex.name = "PulseIndex"

# 4-month lead for plotting vs IPOs
PulseIndex_lead4 = PulseIndex.shift(-4)
PulseIndex_lead4.name = "PulseIndex_lead4"

PulseIndex_lead4 = PulseIndex.shift(-4)
PulseIndex_lead4.name = "PulseIndex_lead4"

# Ensure 'adj_composite_q' column is created in 'align' before use
align['adj_composite_q'] = model.predict(X)

# Define AdjComposite_m by resampling the quarterly adjusted composite to monthly
AdjComposite_m = align['adj_composite_q'].resample('ME').ffill()
AdjComposite_m.name = "AdjComposite_m"

# Lag RawComposite by 4 months
RawComposite_lead4 = RawComposite.shift(-4)
RawComposite_lead4.name = "RawComposite_lead4"

final_pulse_df = pd.concat(
    [components, z_df, AdjComposite_m, PulseIndex, PulseIndex_lead4, RawComposite_lead4],
    axis=1
)

display(final_pulse_df.head())

# Quarterly average of PulseIndex_lead4
pulse_lead4_q = final_pulse_df["PulseIndex_lead4"].resample("QE").mean()

# Align with IPO target
compare = pd.concat([pulse_lead4_q, ipo_count_q], axis=1, join="inner")
compare.columns = ["PulseIndex_lead4_q", "IPO_Activity"]

compare.head()

"""## Timeline"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure and a set of subplots
fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True) # sharex=True ensures x-axis is aligned

# Plot Composite Movement (top subplot)
sns.lineplot(ax=axes[0], x=align.index, y='raw_composite', data=align, color='blue', label='Raw Composite Score')
axes[0].set_title('Raw Composite Score Over Time')
axes[0].set_ylabel('Raw Composite Score')
axes[0].grid(True)
axes[0].legend()

# Plot IPO Activity (bottom subplot)
sns.lineplot(ax=axes[1], x=align.index, y='IPO_Count', data=align, color='green', label='IPO Count')
axes[1].set_title('IPO Activity Over Time')
axes[1].set_xlabel('Pricing Date')
axes[1].set_ylabel('IPO Count')
axes[1].grid(True)
axes[1].legend()

# Adjust layout to prevent overlapping titles/labels
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure and a set of subplots
fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True) # sharex=True ensures x-axis is aligned

# Plot Composite Movement (top subplot)
sns.lineplot(ax=axes[0], x=align.index, y='adj_composite_q', data=align, color='blue', label='Adjusted Composite Score')
axes[0].set_title('Adjusted Composite Score Over Time')
axes[0].set_ylabel('Adjusted Composite Score')
axes[0].grid(True)
axes[0].legend()

# Plot IPO Activity (bottom subplot)
sns.lineplot(ax=axes[1], x=align.index, y='IPO_Count', data=align, color='green', label='IPO Count')
axes[1].set_title('IPO Activity Over Time')
axes[1].set_xlabel('Pricing Date')
axes[1].set_ylabel('IPO Count')
axes[1].grid(True)
axes[1].legend()

# Adjust layout to prevent overlapping titles/labels
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
ax1 = plt.gca()

ax1.plot(compare.index, compare["PulseIndex_lead4_q"], label="PulseIndex (lead 4m)")
ax1.set_xlabel("Date")
ax1.set_ylabel("PulseIndex (2006 = 100)")
ax1.grid(True)

ax2 = ax1.twinx()
ax2.plot(compare.index, compare["IPO_Activity"], linestyle="--", label="IPO Activity (quarterly)")
ax2.set_ylabel("Number of IPOs + Direct Listings")

# Combine legends
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines + lines2, labels + labels2, loc="upper left")

plt.title("PulseIndex (lead 4m) vs Swedish IPO Activity (Quarterly)")
plt.tight_layout()
plt.show()

"""### 2015 onwards"""

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates # Import for date formatting

# Filter the relevant DataFrames for data starting from 2015
align_2015_onwards = align[align.index >= '2015-01-01']
final_pulse_df_2015_onwards = final_pulse_df[final_pulse_df.index >= '2015-01-01']

plt.figure(figsize=(14, 6))

# Create the first y-axis for Raw Composite Score (now with a 4-month lead)
ax1 = plt.gca()
sns.lineplot(x=final_pulse_df_2015_onwards.index, y='RawComposite_lead4', data=final_pulse_df_2015_onwards, color='blue', label='Raw Composite Score (Lead 4m)', ax=ax1)
ax1.set_xlabel('Pricing Date')
ax1.set_ylabel('Raw Composite Score (Lead 4m)', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.grid(True)

# Create a second y-axis that shares the same x-axis for IPO Count
ax2 = ax1.twinx()
sns.lineplot(x=align_2015_onwards.index, y='IPO_Count', data=align_2015_onwards, color='green', label='IPO Count', ax=ax2)
ax2.set_ylabel('IPO Count', color='green')
ax2.tick_params(axis='y', labelcolor='green')

# Set major ticks to be at the start of each year
ax1.xaxis.set_major_locator(mdates.YearLocator())
# Format the date ticks to show just the year
ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
# Ensure all year ticks are visible and potentially rotate them for readability
plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha="right")

# Combine legends from both axes
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines + lines2, labels + labels2, loc='upper left')

plt.title('Raw Composite Score (Lead 4m) and IPO Activity Over Time (2015 Onwards)')
plt.tight_layout()
plt.show()

import statsmodels.api as sm

# Define the independent variable (X) from the 2015 onwards dataframe
X_2015_raw_composite = align_2015_onwards[['raw_composite']]

# Define the dependent variable (y) from the 2015 onwards dataframe
y_2015_ipo_count = align_2015_onwards['IPO_Count']

# Drop rows with NaN values to ensure clean data for regression
df_2015_raw_clean = pd.concat([X_2015_raw_composite, y_2015_ipo_count], axis=1).dropna()

X_2015_raw_clean = df_2015_raw_clean[['raw_composite']]
y_2015_ipo_count_clean = df_2015_raw_clean['IPO_Count']

# Add a constant to the independent variable for the intercept (required by statsmodels)
X_2015_raw_clean = sm.add_constant(X_2015_raw_clean)

# Create and fit the OLS (Ordinary Least Squares) model
model_2015_raw_sm = sm.OLS(y_2015_ipo_count_clean, X_2015_raw_clean)
results_2015_raw_sm = model_2015_raw_sm.fit()

# Print the model summary
print(results_2015_raw_sm.summary())

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(final_pulse_df.index, final_pulse_df["PulseIndex"])
plt.title("Swedish IPO Pulse Index (Monthly)")
plt.xlabel("Date")
plt.ylabel("Index (2006 = 100)")
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Regression"""

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates # Import for date formatting

# Filter the compare DataFrame for data starting from 2015
compare_2015_onwards = compare[compare.index >= '2015-01-01'].copy()

fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True) # Create 2 subplots, 1 column, shared x-axis

# Plot PulseIndex (lead 4m) on the first subplot (axes[0])
sns.lineplot(ax=axes[0], x=compare_2015_onwards.index, y="PulseIndex_lead4_q", color='blue', label="PulseIndex (lead 4m)", data=compare_2015_onwards)
axes[0].set_title('PulseIndex (lead 4m)')
axes[0].set_ylabel('PulseIndex (2006 = 100)')
axes[0].grid(True)
axes[0].legend(loc='upper left')

# Plot IPO Activity (quarterly) on the second subplot (axes[1])
sns.lineplot(ax=axes[1], x=compare_2015_onwards.index, y="IPO_Activity", color='green', label="IPO Activity in Sweden (quarterly)", linestyle="--", data=compare_2015_onwards)
axes[1].set_title('Quarterly IPO Activity in Sweden')
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Number of IPOs + Direct Listings')
axes[1].grid(True)
axes[1].legend(loc='upper left')

# Set major ticks to be at the start of each year
axes[1].xaxis.set_major_locator(mdates.YearLocator())
# Format the date ticks to show just the year
axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
# Ensure all year ticks are visible and potentially rotate them for readability
plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha="right")

plt.tight_layout() # Adjust layout to prevent overlapping titles/labels
plt.show()

# Quarterly average of PulseIndex_lead4
pulse_lead4_q = final_pulse_df["PulseIndex_lead4"].resample("QE").mean()

# Align with IPO target
compare = pd.concat([pulse_lead4_q, ipo_count_q], axis=1, join="inner")
compare.columns = ["PulseIndex_lead4_q", "IPO_Activity"]

# Filter the compare DataFrame for data starting from 2015
compare_2015_onwards = compare[compare.index >= '2015-01-01']

print(compare_2015_onwards.index.max())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x='raw_composite', y='IPO_Count', data=align, label='Actual IPO Count')
sns.lineplot(x='raw_composite', y='adj_composite_q', data=align, color='red', label='Adjusted Composite Score (Predicted IPO Count)')
plt.title('Raw Composite Score vs. IPO Count with Adjusted Composite Trend')
plt.xlabel('Raw Composite Score')
plt.ylabel('IPO Count')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(6, 6))
plt.scatter(compare["PulseIndex_lead4_q"], compare["IPO_Activity"])
plt.xlabel("PulseIndex (lead 4m, quarterly avg)")
plt.ylabel("IPO Activity (quarterly)")
plt.title("Relationship Between PulseIndex and IPO Activity")

# simple correlation in the title
corr = compare["PulseIndex_lead4_q"].corr(compare["IPO_Activity"])
plt.suptitle(f"Correlation: {corr:.2f}", y=0.93, fontsize=9)

plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x='raw_composite', y='IPO_Count', data=align_2015_onwards, label='Actual IPO Count')
sns.lineplot(x='raw_composite', y='adj_composite_q', data=align_2015_onwards, color='red', label='Adjusted Composite Score (Predicted IPO Count)')
plt.title('Raw Composite Score vs. IPO Count with Adjusted Composite Trend')
plt.xlabel('Raw Composite Score')
plt.ylabel('IPO Count')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import statsmodels.api as sm

plt.figure(figsize=(6, 6))
plt.scatter(compare_2015_onwards["PulseIndex_lead4_q"], compare_2015_onwards["IPO_Activity"], label='Actual IPO Activity')

# Define the independent variable (X) from the 2015 onwards dataframe
X_2015 = compare_2015_onwards[['PulseIndex_lead4_q']]

# Define the dependent variable (y) from the 2015 onwards dataframe
y_2015 = compare_2015_onwards['IPO_Activity']

# Drop rows with NaN values from X_2015 and y_2015 to ensure clean data for regression
df_2015_clean = pd.concat([X_2015, y_2015], axis=1).dropna()

X_2015_clean = df_2015_clean[['PulseIndex_lead4_q']]
y_2015_clean = df_2015_clean['IPO_Activity']

# Add a constant to the independent variable for the intercept (required by statsmodels)
X_2015_clean = sm.add_constant(X_2015_clean)

# Create and fit the OLS (Ordinary Least Squares) model
model_2015_sm = sm.OLS(y_2015_clean, X_2015_clean)
results_2015_sm = model_2015_sm.fit()

# Get the predicted values from the statsmodels result
# Need to use the same X_2015_clean that was used for fitting the model
X_for_plot = sm.add_constant(compare_2015_onwards[['PulseIndex_lead4_q']].dropna())
predicted_ipo_activity = results_2015_sm.predict(X_for_plot)

# Sort the X values to plot the line smoothly
sorted_x, sorted_preds = zip(*sorted(zip(X_for_plot['PulseIndex_lead4_q'], predicted_ipo_activity)))

plt.plot(sorted_x, sorted_preds, color='red', label='Regression Line')

plt.xlabel("PulseIndex (lead 4m, quarterly avg)")
plt.ylabel("IPO Activity (quarterly)")
plt.title("Relationship Between PulseIndex and IPO Activity (2015 Onwards)")

# simple correlation in the title
corr = compare_2015_onwards["PulseIndex_lead4_q"].corr(compare_2015_onwards["IPO_Activity"])
plt.suptitle(f"Correlation: {corr:.2f}", y=0.93, fontsize=9)

plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

import statsmodels.api as sm

# Define the independent variable (X) from the 2015 onwards dataframe
X_2015 = compare_2015_onwards[['PulseIndex_lead4_q']]

# Define the dependent variable (y) from the 2015 onwards dataframe
y_2015 = compare_2015_onwards['IPO_Activity']

# Drop rows with NaN values from X_2015 and y_2015 to ensure clean data for regression
df_2015_clean = pd.concat([X_2015, y_2015], axis=1).dropna()

X_2015_clean = df_2015_clean[['PulseIndex_lead4_q']]
y_2015_clean = df_2015_clean['IPO_Activity']

# Add a constant to the independent variable for the intercept (required by statsmodels)
X_2015_clean = sm.add_constant(X_2015_clean)

# Create and fit the OLS (Ordinary Least Squares) model
model_2015_sm = sm.OLS(y_2015_clean, X_2015_clean)
results_2015_sm = model_2015_sm.fit()

# Print the model summary
print(results_2015_sm.summary())

import pandas as pd
import numpy as np
import statsmodels.api as sm

# Retrieve necessary components from kernel state
# RawComposite values are monthly and available up to 2025-11-01
# Using explicit values from the kernel's RawComposite series:
raw_composite_aug_2025 = -0.271932
raw_composite_sept_2025 = -0.282098
raw_composite_oct_2025 = -0.175005
raw_composite_nov_2025 = -0.185024

# base_mean for PulseIndex calculation (from kernel state)
base_mean = 0.4421406086247119

# Calculate PulseIndex for these months
pulse_index_aug_2025 = 100 * raw_composite_aug_2025 / base_mean
pulse_index_sept_2025 = 100 * raw_composite_sept_2025 / base_mean
pulse_index_oct_2025 = 100 * raw_composite_oct_2025 / base_mean
pulse_index_nov_2025 = 100 * raw_composite_nov_2025 / base_mean

# To predict 2026 Q1 (Jan, Feb, Mar 2026) with a 4-month lead:
# We need the average PulseIndex from Sep, Oct, Nov 2025.
pulse_index_predictor_q1_2026 = (pulse_index_sept_2025 + pulse_index_oct_2025 + pulse_index_nov_2025) / 3

print(f"Average PulseIndex for Sep-Nov 2025 (predictor for 2026 Q1): {pulse_index_predictor_q1_2026:.2f}")

# Retrieve the trained model (results_2015_sm) coefficients from the kernel state.
# Note: The model needs to be re-trained with the correct leading logic for PulseIndex.
# This modification assumes the model `results_2015_sm` will be updated prior to this cell's execution.
try:
    # Prepare the input for prediction in the same format as the model's training data.
    # The model should be trained with an added constant: sm.add_constant(X_train).
    X_predict_2026_q1 = pd.DataFrame({'PulseIndex_predictor_q': [pulse_index_predictor_q1_2026]})
    X_predict_2026_q1 = sm.add_constant(X_predict_2026_q1, has_constant='add')

    # Assuming results_2015_sm is available globally and correctly re-trained.
    predicted_ipo_count_2026_q1 = results_2015_sm.predict(X_predict_2026_q1)[0]
    print(f"Predicted IPO count for 2026 Q1: {predicted_ipo_count_2026_q1:.2f}")
except NameError:
    print("\nWarning: 'results_2015_sm' not found. Please ensure the model training cell (mFjdJrCaWsUh) was executed correctly after its update, before running this prediction cell.")
    print("Cannot predict 2026 Q1 without the trained model.")

# Combine PulseIndex and IPO_Count into a single DataFrame
regression_df_pulse_index = pd.concat([final_pulse_df['PulseIndex'], ipo_data_for_q_resampling['IPO_Count']], axis=1)

# Rename columns for clarity
regression_df_pulse_index.columns = ['PulseIndex', 'IPO_Count']

# Filter the DataFrame for data starting from 2015
regression_df_pulse_index_2015_onwards = regression_df_pulse_index[regression_df_pulse_index.index >= '2015-01-01'].copy()

# Drop any rows with NaN values that might result from the merge or filtering
regression_df_pulse_index_2015_onwards.dropna(inplace=True)

# Define the independent variable (X)
X_pulse_index_2015 = regression_df_pulse_index_2015_onwards[['PulseIndex']]

# Define the dependent variable (y)
y_pulse_index_2015 = regression_df_pulse_index_2015_onwards['IPO_Count']

# Add a constant to the independent variable for the intercept (required by statsmodels)
X_pulse_index_2015 = sm.add_constant(X_pulse_index_2015)

# Create and fit the OLS (Ordinary Least Squares) model
model_pulse_index_2015_sm = sm.OLS(y_pulse_index_2015, X_pulse_index_2015)
results_pulse_index_2015_sm = model_pulse_index_2015_sm.fit()

# Print the model summary
print(results_pulse_index_2015_sm.summary())

# Combine PulseIndex and IPO_Count into a single DataFrame
regression_df_pulse_index = pd.concat([final_pulse_df['PulseIndex_lead4'], ipo_data_for_q_resampling['IPO_Count']], axis=1)

# Rename columns for clarity
regression_df_pulse_index.columns = ['PulseIndex_lead4', 'IPO_Count']

# Filter the DataFrame for data starting from 2015
regression_df_pulse_index_2015_onwards = regression_df_pulse_index[regression_df_pulse_index.index >= '2015-01-01'].copy()

# Drop any rows with NaN values that might result from the merge or filtering
regression_df_pulse_index_2015_onwards.dropna(inplace=True)

# Define the independent variable (X)
X_pulse_index_2015 = regression_df_pulse_index_2015_onwards[['PulseIndex_lead4']]

# Define the dependent variable (y)
y_pulse_index_2015 = regression_df_pulse_index_2015_onwards['IPO_Count']

# Add a constant to the independent variable for the intercept (required by statsmodels)
X_pulse_index_2015 = sm.add_constant(X_pulse_index_2015)

# Create and fit the OLS (Ordinary Least Squares) model
model_pulse_index_2015_sm = sm.OLS(y_pulse_index_2015, X_pulse_index_2015)
results_pulse_index_2015_sm = model_pulse_index_2015_sm.fit()

# Print the model summary
print(results_pulse_index_2015_sm.summary())

"""# Visuals"""

print(merged_df_cleaned.columns)
print(df_count_swed.head())

# Select relevant columns from merged_df
df_selected = merged_df[['Pricing Date', 'IPO_Count'] + component_cols].copy()

# Convert 'Pricing Date' to datetime and set as index
df_selected['Pricing Date'] = df_selected['Pricing Date'].dt.to_timestamp()
df_selected = df_selected.set_index('Pricing Date')

# Create a new DataFrame for leading indicators and IPO_Count
df_corr_prep = pd.DataFrame(index=df_selected.index)
df_corr_prep['IPO_Count'] = df_selected['IPO_Count']

# Lead each component by 4 months and add to df_corr_prep
for col in component_cols:
    df_corr_prep[f"{col}_lead4m"] = df_selected[col].shift(-4)

# Calculate the correlation matrix
correlation_matrix = df_corr_prep.corr()

print("Correlation Matrix (IPO_Count vs. 4-month-leading components):")
display(correlation_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix (IPO_Count vs. 4-month-leading components)')
plt.show()

# List of late_components Series (these are already z-scored and monthly)
late_component_series = {
    "z_late_eu": z_late_eu,
    "z_dry_late": z_dry_late,
    "z_pe_nor": z_pe_nor,
    "z_ratio_dry_cap": z_ratio_dry_cap
}

# Ensure IPO_Count is a monthly series with a timestamp index
ipo_count_monthly = ipo_data_for_q_resampling['IPO_Count'].copy()

# Convert ipo_count_monthly index to month-end for consistency
ipo_count_monthly.index = ipo_count_monthly.index + pd.offsets.MonthEnd(0)

# Prepare shifted late components, ensuring timestamp index and dropping NaNs from each shifted series
shifted_late_components = {}
for name, series in late_component_series.items():
    # Ensure series is timestamp indexed
    if isinstance(series.index.dtype, pd.PeriodDtype):
        series_ts_indexed = series.dt.to_timestamp()
    else:
        series_ts_indexed = series.copy() # Already timestamp indexed

    # Apply the shift and drop NaNs *for this individual shifted series*
    # This ensures only the valid shifted range is considered for the inner join
    shifted_series = series_ts_indexed.shift(-4).dropna()
    shifted_series.name = f"{name}_lead4m" # Assign name for concat
    shifted_late_components[shifted_series.name] = shifted_series

# For consistency, also dropna from ipo_count_monthly if there are any internal NaNs
ipo_count_monthly_clean = ipo_count_monthly.dropna()

# Combine IPO_Count with all *cleaned and shifted* late components
# Use concat with join='inner' to align by index and keep only common non-NaN rows at boundaries
df_corr_late_components_prep = pd.concat([ipo_count_monthly_clean] + list(shifted_late_components.values()), axis=1, join='inner')

# Calculate the correlation matrix
correlation_matrix_late = df_corr_late_components_prep.corr()

print("Correlation Matrix (IPO_Count vs. 4-month-leading late_components):")
display(correlation_matrix_late)

# Generate a heatmap for visualization
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_late, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix (IPO_Count vs. 4-month-leading late_components)')
plt.show()

# List of early_components Series (these are already z-scored and monthly)
early_component_series = {
    "z_early_nor": z_early_nor,
    "z_early_eu": z_early_eu,
    "z_dry_early": z_dry_early,
    "z_ratio_early_late": z_ratio_early_late
}

# Ensure IPO_Count is a monthly series with a timestamp index
# Re-using ipo_count_monthly and ipo_count_monthly_clean from previous operations
# Make sure its index is month-end
ipo_count_monthly = ipo_data_for_q_resampling['IPO_Count'].copy()
ipo_count_monthly.index = ipo_count_monthly.index + pd.offsets.MonthEnd(0)
ipo_count_monthly_clean = ipo_count_monthly.dropna()

# Prepare shifted early components, ensuring timestamp index and dropping NaNs from each shifted series
shifted_early_components = {}
for name, series in early_component_series.items():
    # Ensure series is timestamp indexed
    if isinstance(series.index.dtype, pd.PeriodDtype):
        series_ts_indexed = series.dt.to_timestamp()
    else:
        series_ts_indexed = series.copy() # Already timestamp indexed

    # Apply the shift and drop NaNs *for this individual shifted series*
    # This ensures only the valid shifted range is considered for the inner join
    shifted_series = series_ts_indexed.shift(-18).dropna()
    shifted_series.name = f"{name}_lead4m" # Assign name for concat
    shifted_early_components[shifted_series.name] = shifted_series

# Combine IPO_Count with all *cleaned and shifted* early components
# Use concat with join='inner' to align by index and keep only common non-NaN rows at boundaries
df_corr_early_components_prep = pd.concat([ipo_count_monthly_clean] + list(shifted_early_components.values()), axis=1, join='inner')

# Calculate the correlation matrix
correlation_matrix_early = df_corr_early_components_prep.corr()

print("Correlation Matrix (IPO_Count vs. 4-month-leading early_components):")
display(correlation_matrix_early)

# Generate a heatmap for visualization
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_early, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix (IPO_Count vs. 4-month-leading early_components)')
plt.show()